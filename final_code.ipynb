{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0f5ffae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b07347a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(i):\n",
    "    \n",
    "    country_index = pd.read_csv(\"country_index.csv\")\n",
    "    pred_country_index = pd.read_csv(\"submission_country_index.csv\")\n",
    "    \n",
    "    first_index = country_index.loc[i][1]\n",
    "    last_index = country_index.loc[i][2]\n",
    "    tile_value = country_index.loc[i][3]\n",
    "    code = country_index.loc[i][0]\n",
    "    first_index_sub = pred_country_index.loc[i][1]\n",
    "    last_index_sub = pred_country_index.loc[i][2]\n",
    "    print(code)\n",
    "    df1 = pd.read_csv(\"gee_features.csv\", \n",
    "                       skiprows=range(1, 1+first_index), \n",
    "                       nrows=(last_index - first_index + 1))\n",
    "           \n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe67076c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(df1):\n",
    "    \n",
    "    df1 = df1.drop(['new_ind', 'index', 'ADM1DHS', 'ADM1FIPS', 'ADM1FIPSNA', 'ADM1NAME', 'ADM1SALBCO', 'ADM1SALBNA', 'ADM1SALNA', 'ALT_DEM', 'ALT_GPS', 'CCFIPS', 'DATUM', 'DHSCC', 'DHSCLUST', 'DHSREGCO', 'DHSREGNA', 'DHSYEAR', 'F21', 'F22', 'F23', 'LATNUM', 'LONGNUM', 'SOURCE', 'URBAN_RURA', 'ZONECO', 'ZONENA','key1','key2','key3'], axis = 1)\n",
    "    df1 = df1.dropna(axis=1)\n",
    "    df1 = df1.sort_index(axis=1)\n",
    "    dhsid = np.array(df1[\"DHSID\"])\n",
    "\n",
    "    x12 = list(df1.columns)\n",
    "    x12.remove(\"DHSID\")\n",
    "    \n",
    "    df_info = {\n",
    "    'name' : x12,\n",
    "    'len' : [],\n",
    "    'element' : [],\n",
    "    'min' : [],\n",
    "    'max' : [],\n",
    "    'top_value_count' : [],\n",
    "    }\n",
    "    for i in x12:\n",
    "        w1 = list(set(df1[i]))\n",
    "        df_info['max'].append(max(w1))\n",
    "        df_info['min'].append(min(w1))\n",
    "        df_info['element'].append(len(w1))\n",
    "        df_info['len'].append(len(list(df1[i])))\n",
    "        df_info['top_value_count'].append(df1[i].value_counts().values[0])\n",
    "\n",
    "    df_info = pd.DataFrame(df_info) \n",
    "    \n",
    "    \n",
    "    qaw = list(df_info[(df_info['max'] == 65535.000000) & (df_info['element'] > 60)]['name'])\n",
    "    for i in qaw:\n",
    "        sq = np.array(list(df1[i]))\n",
    "        sq[sq == 65535.000000] = np.mean(sq[sq != 65535.000000])\n",
    "        df1[i] = sq\n",
    "        \n",
    "    qaw = list(df_info[(df_info['max'] == 255.000000) & (df_info['element'] > 60)]['name'])\n",
    "    for i in qaw:\n",
    "        sq = np.array(list(df1[i]))\n",
    "        sq[sq == 255.000000] = np.mean(sq[sq != 255.000000])\n",
    "        df1[i] = sq\n",
    "\n",
    "    df_info = {\n",
    "    'name' : x12,\n",
    "    'len' : [],\n",
    "    'element' : [],\n",
    "    'min' : [],\n",
    "    'max' : [],\n",
    "    'top_value_count' : [],\n",
    "    }\n",
    "    for i in x12:\n",
    "        w1 = list(set(df1[i]))\n",
    "        df_info['max'].append(max(w1))\n",
    "        df_info['min'].append(min(w1))\n",
    "        df_info['element'].append(len(w1))\n",
    "        df_info['len'].append(len(list(df1[i])))\n",
    "        df_info['top_value_count'].append(df1[i].value_counts().values[0])\n",
    "\n",
    "    df_info = pd.DataFrame(df_info)\n",
    "    \n",
    "    cat_1 = list(df_info[(df_info['element'] >= 775)]['name'])\n",
    "    cat_2 = list(df_info[(df_info['element'] < 60) & (df_info['element'] != 1)]['name'])\n",
    "    cat_3 = list(df_info[(df_info['element'] < 775) & (df_info['element'] >= 60)]['name'])\n",
    "    \n",
    "    return cat_1, cat_2, cat_3, df1, dhsid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9551a7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def updatearray_reg(X,deg,check):\n",
    "    yset = list(set(X))\n",
    "    yset.sort()\n",
    "    X = np.array(X).reshape(-1)\n",
    "    xset = np.arange(len(yset)).reshape(-1,1)\n",
    "    poly = PolynomialFeatures(degree=deg)\n",
    "    x_poly = poly.fit_transform(xset)\n",
    "    model = LinearRegression()\n",
    "    model.fit(x_poly, yset)\n",
    "    y_pred = model.predict(x_poly)\n",
    "    for i in range(len(yset)):\n",
    "        X[X == yset[i]] = y_pred[i]\n",
    "\n",
    "    if check > 0:\n",
    "        mean = np.mean(X)\n",
    "        std = np.std(X)\n",
    "        X = (X - mean) / std\n",
    "        X = X*check\n",
    "    return X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f932b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_cat(arr):\n",
    "    unique_values = np.unique(arr)\n",
    "    unique_values.sort()\n",
    "    converted_arr = np.searchsorted(unique_values, arr)\n",
    "\n",
    "    return converted_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47568f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_data(cat_1, cat_2, cat_3, df1):\n",
    "    df_cat1 = {}\n",
    "    for i in cat_1:\n",
    "        df_cat1[i]= updatearray_reg(list(df1[i]),2,10)\n",
    "    df_cat1 = pd.DataFrame(df_cat1)\n",
    "    \n",
    "    df_cat2 = {}\n",
    "    for i in cat_2:\n",
    "        df_cat2[i] = unique_cat(list(df1[i]))\n",
    "    for i in cat_3:\n",
    "        df_cat2[i] = updatearray_reg(list(df1[i]),3,5)\n",
    "    df_cat2 = pd.DataFrame(df_cat2)\n",
    "    \n",
    "    df_final = pd.concat([df_cat1, df_cat2], axis=1)\n",
    "    \n",
    "    print(df_final.shape)\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2baf30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_plot_pca(df_final):\n",
    "    data = df_final.values\n",
    "    scaler = StandardScaler()\n",
    "    data = scaler.fit_transform(data)\n",
    "    pca = PCA()\n",
    "    pca.fit(data)\n",
    "    explained_variance_ratio = pca.explained_variance_ratio_\n",
    "    cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "    num_components = len(explained_variance_ratio)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(1, num_components + 1), cumulative_variance_ratio, marker='o', linestyle='-')\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "    plt.title('Cumulative Explained Variance Ratio vs. Number of Components')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fe8a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca_with_standardization(X, j, n_components, dhsid, apply_standardization=False):\n",
    "    back = len(X)\n",
    "    X = np.tile(X, (j, 1))\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    if apply_standardization:\n",
    "        scaler = StandardScaler()\n",
    "        X_pca = scaler.fit_transform(X_pca)\n",
    "        \n",
    "    transformed_df_final = pd.DataFrame(np.array(X_pca[:back]))\n",
    "        \n",
    "    transformed_df_final['DHSID'] = dhsid\n",
    "    df_train = pd.read_csv(\"training_label.csv\")\n",
    "    df_sub = pd.read_csv(\"sample submission (1).csv\")\n",
    "    merged_df = pd.merge(df_train, transformed_df_final, on='DHSID')\n",
    "    merged_df_sub = pd.merge(df_sub, transformed_df_final, on='DHSID')\n",
    "    dhsid_sub = np.array(merged_df_sub[\"DHSID\"])\n",
    "    \n",
    "    df_created_r2 = {\n",
    "    'feature' : list(range(0,n_components)),\n",
    "    'Mean_BMI' : [merged_df[i].corr(merged_df['Mean_BMI']) for i in range(n_components)] ,\n",
    "    'Median_BMI' : [merged_df[i].corr(merged_df['Median_BMI']) for i in range(n_components)],\n",
    "    'Unmet_Need_Rate' : [merged_df[i].corr(merged_df['Unmet_Need_Rate']) for i in range(n_components)],\n",
    "    'Under5_Mortality_Rate' : [merged_df[i].corr(merged_df['Under5_Mortality_Rate']) for i in range(n_components)],\n",
    "    'Skilled_Birth_Attendant_Rate' : [merged_df[i].corr(merged_df['Skilled_Birth_Attendant_Rate']) for i in range(n_components)],\n",
    "    'Stunted_Rate' : [merged_df[i].corr(merged_df['Stunted_Rate']) for i in range(n_components)],\n",
    "    }\n",
    "    df_created_r2 = pd.DataFrame(df_created_r2)\n",
    "        \n",
    "    return merged_df, merged_df_sub, df_created_r2, dhsid_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8171812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data(merged_df, merged_df_sub, df_created_r2, name):\n",
    "    \n",
    "    feature_map = list(df_created_r2[(df_created_r2[name] < -0.03) | (df_created_r2[name] > 0.03)]['feature'])\n",
    "    \n",
    "    f2 = feature_map.copy()\n",
    "    f2.append(name)\n",
    "    sd1 = merged_df[f2]\n",
    "    x_sub = merged_df_sub[feature_map]\n",
    "    \n",
    "    sd2 = sd1.dropna(axis=0)\n",
    "    \n",
    "    X_value = sd2[feature_map]\n",
    "    Y_value = sd2[name]\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(X_value, Y_value, test_size=0.2, random_state=42)\n",
    "    #------------------------------------------------------\n",
    "    from sklearn.linear_model import ElasticNet\n",
    "    ElasticNet = ElasticNet()\n",
    "    param_grid1 = {\n",
    "        'alpha': [0.1, 0.15, 0.2, 0.25,0.3,0.35, 0.4,0.45, 0.5,0.55, 0.6,0.65, 0.7,0.75,0.8,0.85, 0.9,1],\n",
    "        'l1_ratio': [0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    }\n",
    "    grid_search1 = GridSearchCV(estimator=ElasticNet, param_grid=param_grid1, scoring='r2', cv=5)\n",
    "    grid_search1.fit(x_train, y_train)\n",
    "    best_model1 = grid_search1.best_estimator_\n",
    "    best_params1 = grid_search1.best_params_\n",
    "    elastic_net_preds = best_model1.predict(x_test)\n",
    "    accuracy1 = np.sqrt(mean_squared_error(y_test, elastic_net_preds))\n",
    "    \n",
    "    #------------------------------------------------------\n",
    "    linear_reg = LinearRegression()\n",
    "    param_grid2 = {\n",
    "    'fit_intercept': [True, False],\n",
    "    'normalize': [True, False]\n",
    "    }\n",
    "    grid_search2 = GridSearchCV(estimator=linear_reg, param_grid=param_grid2, scoring='r2', cv=5)\n",
    "    grid_search2.fit(x_train, y_train)\n",
    "    best_model2 = grid_search2.best_estimator_\n",
    "    best_params2 = grid_search2.best_params_\n",
    "    linear_reg_preds = best_model2.predict(x_test)\n",
    "    accuracy2 = np.sqrt(mean_squared_error(y_test, linear_reg_preds))\n",
    "    \n",
    "    #------------------------------------------------------\n",
    "    values = [accuracy1, accuracy2]\n",
    "    minimum_value = min(values)\n",
    "    p = values.index(minimum_value)\n",
    "    minx = min(list(Y_value))\n",
    "    manx = max(list(Y_value))\n",
    "    \n",
    "    if p == 0:\n",
    "        y_pred_sub = best_model1.predict(x_sub)\n",
    "        info = accuracy1    \n",
    "        return info, y_pred_sub, minx, manx\n",
    "    if p == 1:\n",
    "        y_pred_sub = best_model2.predict(x_sub)\n",
    "        info = accuracy2\n",
    "        return info, y_pred_sub, minx, manx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbf5e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1= import_data(50)\n",
    "cat_1, cat_2, cat_3, df1, dhsid = filter_data(df1)\n",
    "df_final = final_data(cat_1, cat_2, cat_3, df1)\n",
    "show_plot_pca(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c936664f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df, merged_df_sub, df_created_r2, dhsid_sub = apply_pca_with_standardization(df_final, 2, 1000, dhsid, apply_standardization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2293c6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['Mean_BMI','Median_BMI','Unmet_Need_Rate','Under5_Mortality_Rate','Skilled_Birth_Attendant_Rate', 'Stunted_Rate']\n",
    "\n",
    "y_p = []\n",
    "rmse = []\n",
    "max_value1 = []\n",
    "y_max_min = []\n",
    "for name in names:\n",
    "    info, y_pred_sub, minx, manx = train_data(merged_df, merged_df_sub, df_created_r2, name)\n",
    "    y_pred_sub = list(y_pred_sub)\n",
    "    y_abs = [abs(num) for num in y_pred_sub]\n",
    "    max_value = max(y_abs)\n",
    "    max_value1.append(max_value)\n",
    "    y_p.append(y_pred_sub)\n",
    "    rmse.append(info)\n",
    "    y_max_min.append([minx,manx])\n",
    "    \n",
    "dic = {\n",
    "     \"dhsid\": list(dhsid_sub),\n",
    "     \"mean\": y_p[0],\n",
    "     \"median\": y_p[1],\n",
    "     \"unmet\": y_p[2],\n",
    "     \"under5\": y_p[3],\n",
    "     \"skilled\": y_p[4],\n",
    "     \"stunt\": y_p[5]\n",
    " }   \n",
    "                \n",
    "dic = pd.DataFrame(dic)     \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b560d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c00959d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mrmse = np.mean(rmse)\n",
    "mrmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b6c225",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic.to_csv(\"new.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a15db04",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_value1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7002e961",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbebe00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c82a81a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
